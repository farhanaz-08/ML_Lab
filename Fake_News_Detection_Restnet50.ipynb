{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoAs/0w2v1MxEf1FDqIC1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhanaz-08/ML_Lab/blob/main/Fake_News_Detection_Restnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone Ashia's repo"
      ],
      "metadata": {
        "id": "SX0cEshLwvjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fS5me3zStu3c",
        "outputId": "b418c9d3-3e8e-4931-cfbb-2b955cc9a790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Fake-News-Detection-Bilingual-Multimodel'...\n",
            "remote: Enumerating objects: 10046, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 10046 (delta 1), reused 1 (delta 0), pack-reused 10041 (from 3)\u001b[K\n",
            "Receiving objects: 100% (10046/10046), 1.72 GiB | 16.40 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n",
            "Updating files: 100% (13988/13988), done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!git clone https://github.com/ashia-002/Fake-News-Detection-Bilingual-Multimodel.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm repo is cloned"
      ],
      "metadata": {
        "id": "WIRcXJsmw2O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1NKz6_Hu_IX",
        "outputId": "86ad9824-5ce1-42be-fa19-42c574f83cf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake-News-Detection-Bilingual-Multimodel  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking dataset location inside the repo"
      ],
      "metadata": {
        "id": "g99e_QkcxEXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Fake-News-Detection-Bilingual-Multimodel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP_SwVhlvXep",
        "outputId": "fe749291-e354-4ca9-b0c4-5a3bf72594ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AI_Lab01.ipynb   file\t\t\t\t   logs        README.md\n",
            " CNN_Pipeline\t 'image Clasification CNN.ipynb'   models      src\n",
            " data\t\t  LargeDataset\t\t\t   notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting required dataset path"
      ],
      "metadata": {
        "id": "YClwE-sNxMZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# adjust this path based on actual structure\n",
        "dataset_path = \"/content/Fake-News-Detection-Bilingual-Multimodel/data\"\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "print(\"Total images:\", len(full_dataset))\n",
        "print(\"Classes:\", full_dataset.classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEw1ZfKbvkLy",
        "outputId": "66c0233c-49cd-4df8-b956-60df131bd39f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 4000\n",
            "Classes: ['fake', 'real']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the Dataset\n",
        "70% training\n",
        "20% validation\n",
        "10% testing"
      ],
      "metadata": {
        "id": "ySwhZ8J_xZb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Define split sizes\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "# Split dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"âœ… Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "-0Nf08AJwrpI",
        "outputId": "93f67ed7-f847-498f-9978-cc917d24f232"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'full_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1841165318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define split sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'full_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "rljgnesLzLUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n"
      ],
      "metadata": {
        "id": "qB_oibQmzOQ_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Pre-trained RestNet-50"
      ],
      "metadata": {
        "id": "Ha9A5W4xzRdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = models.resnet50(weights='IMAGENET1K_V1')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq1MbNqLzWnh",
        "outputId": "7d72f9fd-179e-45bf-e02e-74aa036faec5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 220MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing early layers"
      ],
      "metadata": {
        "id": "gvgJutLrzhM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in resnet50.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "MlUtQ6SJztuw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing classification layer to 2"
      ],
      "metadata": {
        "id": "6I6vtHpKzwx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = resnet50.fc.in_features\n",
        "resnet50.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, 2),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZKlikh8s0HIl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining loss & optimizer"
      ],
      "metadata": {
        "id": "mOuQFZa90KC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(resnet50.fc.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "id": "HtG5epXn0O8f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving model to GPU"
      ],
      "metadata": {
        "id": "kA0HoLeG0Uf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet50.to(device)\n",
        "print(\"Training on:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mfe3fkd0Z9C",
        "outputId": "16c50802-ebf1-492a-dc1f-50c6ee4284ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and validating the model"
      ],
      "metadata": {
        "id": "2cawXkmh0mnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    resnet50.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # ðŸ”¹ Training with progress bar\n",
        "    train_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{epochs}] Training\", leave=False)\n",
        "    for images, labels in train_bar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet50(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # ðŸ”¹ Validation with progress bar\n",
        "    resnet50.eval()\n",
        "    val_loss, val_correct = 0.0, 0\n",
        "    val_bar = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{epochs}] Validation\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = resnet50(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "\n",
        "            # Update validation bar\n",
        "            val_bar.set_postfix(val_loss=loss.item())\n",
        "\n",
        "    # ðŸ”¹ Summary per epoch\n",
        "    print(f\"âœ… Epoch [{epoch+1}/{epochs}] \"\n",
        "          f\"Train Loss: {running_loss/len(train_loader):.4f} \"\n",
        "          f\"Val Loss: {val_loss/len(val_loader):.4f} \"\n",
        "          f\"Val Acc: {val_correct/len(val_dataset):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "collapsed": true,
        "id": "mW1CqwRW0lGj",
        "outputId": "4b6aed05-c393-4765-98f5-d12cc7cdc343"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-938677398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# ðŸ”¹ Training with progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{epochs}] Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    }
  ]
}